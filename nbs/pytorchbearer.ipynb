{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch (1.3.0)\n",
      "torchvision (0.4.1)\n",
      "torchbearer (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import torchbearer\n",
    "\n",
    "from playground_utils import * # get common imports / vars for all nbs\n",
    "all_module_versions(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "save_path = Path('~/Documents/data/torchvision/')\n",
    "\n",
    "DEVICE = print_and_return( torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') )\n",
    "NUM_WORKERS = 0 # print_and_return( os.cpu_count() )\n",
    "BATCH_SIZE  = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "45000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "normalize  = torchvision.transforms.Normalize(**IMAGENET_NORMALIZATION)\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                             normalize])\n",
    "\n",
    "# DATASETS\n",
    "ds = torchvision.datasets.CIFAR10(str(save_path), train=True,  download=True, transform=transforms)\n",
    "splitter = torchbearer.cv_utils.DatasetValidationSplitter(len(ds), 0.1)\n",
    "\n",
    "train_ds = splitter.get_train_dataset(ds)\n",
    "valid_ds = splitter.get_val_dataset(ds)\n",
    "test_ds  = torchvision.datasets.CIFAR10(str(save_path), train=False, download=True, transform=transforms)\n",
    "\n",
    "# DATALOADERS\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dl  = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "classes = ds.classes\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(valid_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45000/45000 [00:06<00:00, 6549.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'frog': 4468,\n",
       "         'truck': 4510,\n",
       "         'ship': 4497,\n",
       "         'automobile': 4531,\n",
       "         'bird': 4506,\n",
       "         'airplane': 4537,\n",
       "         'horse': 4504,\n",
       "         'deer': 4498,\n",
       "         'dog': 4494,\n",
       "         'cat': 4455})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_distribution(train_ds, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train_ds.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
